# Hadoop搭建

# 1.虚拟机安装

> 1.安装虚拟机,IP地址192.168.10.100、主机名称hadoop100、内存4G、硬盘50G

修改子网和网关保持在同一网段中

![image-20230817175323258](img.assets\image-20230817175323258.png)

![image-20230817175411591](img.assets\image-20230817175411591.png)



> 2.改为静态IP

`vim /etc/sysconfig/network-scripts/ifcfg-ens33`进入配置文件

```
BOOTPROTO="static"
…
IPADDR=1192.168.79.100
GATEWAY=192.168.79.2
DNS1=192.168.79.2
```

> 3.建立主机名映射

- Linux系统进入配置文件`vim /etc/hosts`，加上主机名及其对应IP地址 或者 `echo "192.168.79.100 hadoop100" >> /etc/hosts`
- 如果主机名不是hadoop100可通过`echo "hadoop100" > /etc/hostname`来修改
- 再进windows系统C:\Windows\System32\drivers\etc同样修改hosts文件



> 4.安装软件包

```
yum install -y epel-release
```

Extra Packages for Enterprise Linux是为“红帽系”的操作系统提供额外的软件包

- net-tool：工具包集合，包含 ifconfig 等命令

```
yum install -y net-tools
```

- vim：编辑器

```
yum install -y vim
```



>5.关闭防火墙及其开机自启

- 关闭防火墙：`systemctl stop firewalld`
- 关闭开机自启：`systemctl disable firewalld.service`



> 6.不是root权限的添加root权限

- root`用户`vim /etc/sudoers`第91行加上`该用户名 ALL=(ALL) NOPASSWD: ALL`，由于是只读文件，退出时`wq!
- 将用户`usermod -g wheel 该用户名` 添加到wheel组中，加上`NOPASSWD`即可



> 7.在/opt目录下创建文件夹

- 创建文件夹(JDK和Hadoop安装目录)

```
mkdir /opt/module
mkdir /opt/software
```

- 修改所有者、所属组(选择root用户登录的话可以不修改)

```
chown xh:xh /opt/module
chown xh:xh /opt/software
```



> 8.卸载虚拟机自带的JDK

- 先检查虚拟机是否自带了JDK：`rpm -qa | grep -i java`
- 一键删除操作：`rpm -qa | grep -i java | xargs -n1 rpm -e --nodeps`



> 9.重启 reboot



# 2.克隆三台虚拟机

拷贝虚拟机,IP地址**192.168.79.102 Hadoop102**,**192.168.79.103 Hadoop103**,**192.168.79.104 Hadoop104**

设置ip和主机名映射，保持三台主机可以通过主机名连接

```
192.168.79.102 hadoop102
192.168.79.103 hadoop103
192.168.79.104 hadoop104
```

> 配置windos主机映射





# 3.安装JDK和Hadoop

在Hadoop102上安装JDK和Hadoop到`/opt/module`目录,其余选择拷贝的方式安装



>Hadoop安装

```shell
tar -zxvf hadoop-3.1.3.tar.gz -C /opt/module/
```

修改配置文件

```shell
vim /etc/profile
```

```sh
#HADOOP_HOME
export HADOOP_HOME=/opt/module/hadoop-3.1.3
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
```

刷新当前的shell环境

```shell
source /etc/profile
```

最后查看是否成功安装

```
hadoop
```

>其中重要目录：
>
>bin 目录：存放对 Hadoop 相关服务（ HDFS,YARN）进行操作的脚本
>etc 目录： Hadoop 的配置文件目录，存放 Hadoop 的配置文件
>lib 目录：存放 Hadoop 的本地库（对数据进行压缩解压缩功能）
>sbin 目录：存放启动或停止 Hadoop 相关服务的脚本；share 目录：存放 Hadoop 的依赖 jar 包、 文档和官方案例



# 4.Hadoop运行模式

[Hadoop官方网站](https://hadoop.apache.org/)

Hadoop 运行模式包括：本地模式、伪分布式模式以及完全分布式模式

- `本地（独立）模式`：无需运行任何守护进程，所有程序都在同一个JVM上执行，在独立模式下测试和调试MapReduce都非常方便，因此该模式在开发阶段比较适合（学习用）
- `伪分布式模式`：Hadoop守护进程运行在本地机器上，模拟一个小规模的集群
- `完全分布式`：Hadoop守护进程运行在一个集群上（企业用)

![image-20230818143928626](img.assets\image-20230818143928626.png)

## 4.1 本地运行模式（官方 WordCount）

>在 hadoop-3.1.3 文件下面创建一个 wcinput 文件夹

```
mkdir wcinput
# 进入文件夹中
cd wcinput
```

>编辑word.txt文本

```
vim word.txt
# 操作如下
11 11 11
22 22
33
```

> 回到Hadoop目录下执行

```java
hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount wcinput wcoutput
```

- wordcount 统计文本数量
- wcinput 输入路径
- wcoutput 输出路径(输出路径不能存在，否则回抛出异常)

> 查看输出文件

![image-20230818145607097](img.assets\image-20230818145607097.png)

> 查看结果

```shell
cat part-r-00000
```

![image-20230818150255320](img.assets\image-20230818150255320.png)

> 异常信息

![image-20230818150015167](img.assets\image-20230818150015167.png)



## 4.2 完全分布式运行模式

- 准备 3 台客户机（关闭防火墙、静态 IP、主机名称）
- 安装 JDK
- 配置环境变量
- 安装 Hadoop
- 配置环境变量
- 配置集群
- 单点启动
- 配置 ssh
- 群起并测试集群



### 4.2.1 scp 安全拷贝

🎯语法：`scp [选项] 要拷贝的文件路径/名称 目的用户@主机:目的路径/名称`

> scp -r   $pdir/$fname       $user@$host:$pdir/$fname 
>
> 命令 递归 要拷贝的文件路径/名称 目的地用户@主机:目的地路径/名称

🎯功能：可以实现服务器之间的数据拷贝

🎯选项：

| 选项 | 说明                                                |
| ---- | --------------------------------------------------- |
| -C   | 这会在复制过程中压缩文件或目录                      |
| -P   | 如果默认 SSH 端口不是 22，则使用此选项指定 SSH 端口 |
| -r   | 此选项递归复制目录及其内容                          |
| -p   | 保留文件的访问和修改时间                            |

🎯例子：

前提：在 hadoop102、hadoop103、hadoop104 都已经创建好的/opt/module、  /opt/software 两个目录，并且已经把这两个目录修改为xh:xh

```shell
# 文件夹在hadoop102上，拷贝到其他虚拟机上
# 102 --> 103
scp -r /opt/module/jdk1.8.0_212  root@192.168.79.103:/opt/module
# 103 <-- 102
scp -r root@192.168.79.102:/opt/module/hadoop/ /opt/module/
# (103)102 --> 104
scp -r /opt/module/*  root@192.168.79.104:/opt/module/
```

```
上面命令将本地目录xxx复制到远程 module 目录下，会创建子目录
```



### 4.2.2 rsync 远程同步工具

> rsync 主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点

🎯rsync 和 scp 区别：用 rsync 做文件的复制要比 scp 的速度快，**rsync 只对差异文件做更新**。scp 是把所有文件都复制过去
🎯语法：`rsync [选项] 要拷贝的文件路径/名称 目的用户@主机:目的路径/名称`

>rsync  -av 	$pdir/$fname 		$user@$host:$pdir/$fname 
>
>命令  选项参数   要拷贝的文件路径/名称   目的地用户@主机:目的地路径/名称

🎯选项：

| 选项 | 功能         |
| ---- | ------------ |
| -a   | 归档拷贝     |
| -v   | 显示复制过程 |
| -l   | 拷贝符号链接 |

🎯例子：

```shell
# 更新同步hadoop103里面hadoop内容
rsync -av hadoop/ root@192.168.79.103:/opt/module/hadoop/
```

>**注意**：
>
>1. 如果只想同步源目录source里面的内容到目录destination，则需要在源目录后面加上斜杠
>2. 上面命令执行后，source目录里面的内容，就都被复制到了destination目录里面，不会在destination下面创建一个source子目录



### 4.2.3 xsync 集群分发脚本

🎯需求：循环复制文件到所有节点的相同目录下
🎯期待脚本：`xsync 要同步的文件名称`
🎯将脚本放在声明了全局环境变量的路径中

> 查看全局环境变量路径

```
echo $PATH
```

> /usr/local/sbin:
>
> /usr/local/bin:
>
> /usr/sbin:/usr/bin:
>
> /root/bin: #在root目录下的bin目录中可以全局使用
>
> /opt/module/jdk1.8.0_212/bin:
>
> /opt/module/jdk1.8.0_212/bin:
>
> /opt/module/jdk1.8.0_212/bin:
>
> /opt/module/hadoop-3.1.3/bin:
>
> /opt/module/hadoop-3.1.3/sbin

**创建`xsync`脚本:**

>发现`/root/bin`路径，故选择在根目录下创建`bin`目录，创建脚本`xsync`全局可用
>
>vim xsync

```sh
#!/bin/bash
#1. 判断参数个数
if [ $# -lt 1 ]
then
	echo Not Enough Arguement!
	exit;
fi
#2. 遍历集群所有机器
for host in 192.168.79.102 192.168.79.103 192.168.79.104
do
	echo ==================== $host ====================
 	#3. 遍历所有目录，挨个发送
 	for file in $@
 	do
  		#4. 判断文件是否存在
  		if [ -e $file ]
   			then
			     #5. 获取父目录
				 pdir=$(cd -P $(dirname $file); pwd)
				 #6. 获取当前文件的名称
				 fname=$(basename $file)
				 ssh $host "mkdir -p $pdir"
				 rsync -av $pdir/$fname $host:$pdir
			else
			     echo $file does not exists!
		fi
 	done
done
```

> 设置可执行权限

```
chmod +x xsync
```

> 测试

```shell
xsync /root/bin
```

>同步环境变量配置（root 所有者）
>
>**注意：如果用了 sudo，那么 xsync 一定要给它的路径补全。**  --> `sudo ./bin/xsync`

```
xsync /etc/profile
```

让环境变量生效

```
source /etc/profile
```



### 4.2.4 ssh 无密码登录配置

> 配置ssh

基本语法

**`ssh 另一台电脑的 IP 地址`**

![image-20230818155929781](img.assets\image-20230818155929781.png)

> 无密钥配置

🎯免密登录原理

![image-20230818160026919](img.assets\image-20230818160026919.png)

> 进入到/root目录，只要执行过`ssh命令`,在隐藏目录下就有应该`.ssh目录`

![image-20230818160651886](img.assets\image-20230818160651886.png)

> 查看目录

![image-20230818160820602](img.assets\image-20230818160820602.png)

> 在`.ssh`目录下生成公钥和私钥

```
ssh-keygen -t rsa
```

然后敲（三个回车），就会生成两个文件 id_rsa（私钥）、id_rsa.pub（公钥）

![image-20230818160959891](img.assets\image-20230818160959891.png)

> 将公钥拷贝到要免密登录的目标机器上

```
ssh-copy-id 192.168.79.102
ssh-copy-id 192.168.79.103
ssh-copy-id 192.168.79.104
```

![image-20230818161148887](img.assets\image-20230818161148887.png)

> 查看文件`authorized_keys`

![image-20230818161344109](img.assets\image-20230818161344109.png)

> 在其他服务器上执行相同的操作



### 4.2.5 Hadoop集群配置

>NameNode 和 SecondaryNameNode 不要安装在同一台服务器，很消耗内存，
>
>ResourceManager 也很消耗内存，不要和 NameNode、SecondaryNameNode 配置在同一台机器上

🎯集群配置部署

|      | 192.168.79.102 hadoop102   | 192.168.79.103 hadoop103             | 192.168.79.104 hadoop104            |
| ---- | -------------------------- | ------------------------------------ | ----------------------------------- |
| HDFS | **`NameNode`**<br>DataNode | <br>DataNode                         | **`SecondaryNameNode`**<br>DataNode |
| YARN | NodeManager                | **`ResourceManager`**<br>NodeManager | <br>NodeManager                     |

🎯配置文件说明：Hadoop 配置文件分两类，**默认配置文件**和**自定义配置文件**，只有用户想修改某一默认配置值时，才需要修改自定义配置文件，更改相应属性值

> 默认配置文件

| 默认配置文件       | 文件存放在 Hadoop 的 jar 包中的位置                       |
| ------------------ | --------------------------------------------------------- |
| core-default.xml   | hadoop-common-3.1.3.jar/core-default.xml                  |
| hdfs-default.xml   | hadoop-hdfs-3.1.3.jar/hdfs-default.xml                    |
| yarn-default.xml   | hadoop-yarn-common-3.1.3.jar/yarn-default.xml             |
| mapred-default.xml | hadoop-mapreduce-client-core-3.1.3.jar/mapred-default.xml |

>自定义配置文件（常用）

`core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml`、`workers` 五个配置文件存放在`/opt/module/hadoop-3.1.3/etc/hadoop`这个路径上，用户可以根据项目需求重新进行修改配置

![image-20230818165403796](img.assets\image-20230818165403796.png)

![image-20230818165511786](img.assets\image-20230818165511786.png)

以下配置在  192.168.79.102 hadoop102 中进行

#### 4.2.5.1 core-site.xml 

🎯核心配置文件：`vim core-site.xml`，相当于内部通讯，在`<configuration>`和`</configuration>`之间插入

```xml
    <!-- 指定NameNode的地址 -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://192.168.79.102:8020</value>
    </property>

    <!-- 指定hadoop数据的存储目录，根据自身情况做出更改 -->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/opt/module/hadoop-3.1.3/data</value>
    </property>

    <!-- 配置HDFS网页登录使用的静态用户为root -->
    <property>
        <name>hadoop.http.staticuser.user</name>
        <value>root</value>
    </property>
```



#### 4.2.5.2 hdfs-site.xml

🎯HDFS配置文件：外部通讯的接口，`vim hdfs-site.xml`，在`<configuration>`和`</configuration>`之间插入以下内容

```xml
     <!-- nn web端访问地址-->
    <property>
        <name>dfs.namenode.http-address</name>
        <value>192.168.79.102:9870</value>
    </property>
        <!-- 2nn web端访问地址-->
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>192.168.79.104:9868</value>
    </property>
```



#### 4.2.5.3 yarn-site.xml

🎯YARN配置文件：`vim yarn-site.xml`，在`<configuration>`和`</configuration>`之间插入以下内容

```xml
    <!-- 指定MR走shuffle -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>

    <!-- 指定ResourceManager的地址-->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>192.168.79.103</value>
    </property>

    <!-- 环境变量的继承 解决3.1.3版本的小bug -->
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
                <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
```



#### 4.2.5.4 mapred-site.xml

🎯MapReduce配置文件：`mapred-site.xml`,在`<configuration>`和`</configuration>`之间插入以下内容

```xml
<!-- 指定MapReduce程序运行在Yarn上 -->
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
```



#### 4.2.5.5 workers

🎯配置workers：`vim /opt/module/hadoop-3.1.3/etc/hadoop/workers `，增加以下内容（该文件中添加的内容结尾不允许有空格，文件中不允许有空行）

```
192.168.79.102
192.168.79.103
192.168.79.104
```



#### 4.2.5.6 分发到其他虚拟机

```
xsync hadoop/
```



### 4.2.6 启动集群

🎯启动集群：如果集群是第一次启动，需要在 192.168.79.102 hadoop102 节点格式化 NameNode

```
# 在hadoop根目录输入
hdfs namenode -format
# 随后会产生data和logs两个文件
```

>格式化 NameNode，会产生新的集群 id，导致 NameNode 和 DataNode 的集群 id 不一致，集群找不到已往数据。如果集群在运行过程中报错，需要重新格式化 NameNode 的话，一定要先停止 namenode 和 datanode 进程，并且要删除所有机器的 data 和 logs 目录，然后再进行格式化
>
>- 杀进程sbin/stop-dfs.sh
>- 删除每个虚拟机上的logs和data
>- 格式化NameNode：hdfs namenode -format

![image-20230821093217625](img.assets\image-20230821093217625.png)

在 /opt/module/hadoop-3.1.3/data/dfs/name/current 有一个 `VERSION `,保存着集群id

![image-20230821093354881](img.assets\image-20230821093354881.png)

#### 4.2.6.1 启动HDFS

在hadoop的`sbin`目录下执行

```
start-dfs.sh
```

🎯查看是否启动成功：

```
# jps查看启动的进程(Java进程的相关信息)
jps
```

🤯启动hadoop集群时还有可能会报如下错误

```
Starting namenodes on [hadoop102]
ERROR: Attempting to operate on hdfs namenode as root
ERROR: but there is no HDFS_NAMENODE_USER defined. Aborting operation.
Starting datanodes
ERROR: Attempting to operate on hdfs datanode as root
ERROR: but there is no HDFS_DATANODE_USER defined. Aborting operation.
Starting secondary namenodes [192.168.79.104]
ERROR: Attempting to operate on hdfs secondarynamenode as root
ERROR: but there is no HDFS_SECONDARYNAMENODE_USER defined. Aborting operation.
```

🎯解决方案：

```sh
vim /etc/profile
# 在环境变量中添加下面的配置
export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root

#刷新配置
source /etc/profile
xsync /etc/profile
```



🤯出现JAVA_HOME无法找到的问题

```
hadoop101: ERROR: JAVA_HOME is not set and could not be found.
```

找到安装Hadoop的位置，在hadoop安装的目录下有个hadoop-env的文件。

![image-20230821113011092](img.assets\image-20230821113011092.png)

在`export JAVA_HOME=`的地方添加安装的jdk的绝对路径

![image-20230821115904142](img.assets\image-20230821115904142.png)



#### 4.2.6.2 启动YARN

🎯启动YARN（192.168.79.103 hadoop103 上启动）

```
# 同样在hadoop根目录下
sbin/start-yarn.sh

# jps查看启动的进程
jps
```



#### 4.2.6.3 web查看

> web端查看 `HDFS` 的 NameNode：浏览器中输入 http://192.168.79.102:9870

![image-20230821122531617](img.assets\image-20230821122531617.png)

> Web 端查看`YARN` 的 ResourceManager：浏览器中输入http://192.168.79.103:8088

![image-20230821122556513](img.assets\image-20230821122556513.png)



#### 4.2.6.4 集群测试

```
# 创建一个目录
hadoop fs -mkdir /input
# 上传文件
hadoop fs -put wcinput/word.txt /input
```

![image-20230821141339149](img.assets\image-20230821141339149.png)

> 下载查看需要配置主机名和ip地址的映射

查看**上传文件后查看文件存放的位置**

HDFS的存储路径：

![image-20230821143123283](img.assets\image-20230821143123283.png)

查看HDFS在磁盘存储文件内容：`cat blk_1073741825`，可以发现`blk_一长串数字`即我们上传的文件，可以进行查看、解压等操作

🎯测试YARN（需要计算时才会显示任务）：

```sh
# hadoop根目录下
hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output
```

![image-20230821150255812](img.assets\image-20230821150255812.png)

> 在HDFS页面中查看计算结果

![image-20230821150515388](img.assets\image-20230821150515388.png)



#### 4.2.6.5 配置历史服务器

为了查看程序的历史运行情况，需要配置一下历史服务器

🎯配置mapred-site.xml

```
vim /opt/module/hadoop-3.1.3/etc/hadoop/mapred-site.xml
```

```xml
<!-- 历史服务器端地址(供内部) -->
<property>
	<name>mapreduce.jobhistory.address</name>
	<value>192.168.79.102:10020</value>
</property>
<!-- 历史服务器 web 端地址（供外部） -->
<property>
	<name>mapreduce.jobhistory.webapp.address</name>
	<value>192.168.79.102:19888</value>
</property>
```

🎯回到`/opt/module/hadoop-3.1.3/etc/hadoop`分发配置

```
xsync mapred-site.xml
```

>!重启yarn `stop-yarn.sh`

192.168.79.102 hadoop102上启动历史服务器

```
sbin/mapred --daemon start historyserver
```

查看是否启动

```
jps
```

![image-20230821152956699](img.assets\image-20230821152956699.png)

查看历史服务器

![image-20230821153640343](img.assets\image-20230821153640343.png)



#### 4.2.6.6 配置日志的聚集

![image-20230821154027546](img.assets\image-20230821154027546.png)

点击logs发现

![image-20230821154107658](img.assets\image-20230821154107658.png)

没有配置日志的聚集功能

**日志聚集概念**：应用运行完成以后，将程序运行日志信息上传到HDFS系统上，可以方便的查看到程序运行详情，方便开发调试

![image-20230821154245327](img.assets\image-20230821154245327.png)

开启日志聚集功能，需要**重新启动**NodeManager、ResourceManager和HistoryServer

> 🎯配置yarn-site.xml

vim /opt/module/hadoop-3.1.3/etc/hadoop/yarn-site.xml

```xml
<!-- 开启日志聚集功能 -->
<property>
	<name>yarn.log-aggregation-enable</name>
	<value>true</value>
</property>
<!-- 设置日志聚集服务器地址 -->
<property> 
	<name>yarn.log.server.url</name> 
	<value>http://192.168.79.102:19888/jobhistory/logs</value>
</property>
<!-- 设置日志保留时间为 7 天 -->
<property>
	<name>yarn.log-aggregation.retain-seconds</name>
	<value>604800</value>
</property>
```

🎯保存之后分发

```
xsync yarn-site.xml
```

🎯重启NodeManager 、ResourceManager 和 HistoryServer，单个服务停止

```
mapred --daemon stop historyserver
```

切换至 192.168.79.103 hadoop103上

```
sbin/stop-yarn.sh   sbin/start-yarn.sh
```

切回192.168.79.102 hadoop102

```
mapred --daemon start historyserver
```

🎯测试

![image-20230821155805019](img.assets\image-20230821155805019.png)



#### 4.2.6.7 集群启动/停止方式

🎯各个**模块**分开启动/停止（配置 ssh 是前提）

```sh
# 整体启动/停止HDFS
start-dfs.sh
stop-dfs.sh
# 整体启动/停止yarn（hadoop103）
start-yarn.sh
stop-yarn.sh
```

🎯各个**服务组件**逐一启动/停止

```sh
# 单独启动/停止HDFS组件（namenode或datanode或secondarynamenode其中之一）
hdfs --daemon start namenode或datanode或secondarynamenode
hdfs --daemon stop namenode或datanode或secondarynamenode
# 单独启动/停止YARN组件
yarn --daemon stop resourcemanager或nodemanager
yarn --daemon stop resourcemanager或nodemanage
```

🎯注意到上述操作繁杂重复，故可以用**脚本一键**启动/关停所有`vim /root/bin/myhadoop.sh`：

```sh
#!/bin/bash
if [ $# -lt 1 ]
then
 echo "No Args Input..."
 exit ;
fi
case $1 in
"start")
 echo " =================== 启动 hadoop 集群 ==================="
 echo " --------------- 启动 hdfs ---------------"
 ssh 192.168.79.102 "/opt/module/hadoop-3.1.3/sbin/start-dfs.sh"
 echo " --------------- 启动 yarn ---------------"
 ssh 192.168.79.103 "/opt/module/hadoop-3.1.3/sbin/start-yarn.sh"
 echo " --------------- 启动 historyserver ---------------"
 ssh 192.168.79.102 "/opt/module/hadoop-3.1.3/bin/mapred --daemon start historyserver"
;;
"stop")
 echo " =================== 关闭 hadoop 集群 ==================="
 echo " --------------- 关闭 historyserver ---------------"
 ssh 192.168.79.102 "/opt/module/hadoop-3.1.3/bin/mapred --daemon stop historyserver"
 echo " --------------- 关闭 yarn ---------------"
 ssh 192.168.79.103 "/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh"
 echo " --------------- 关闭 hdfs ---------------"
 ssh 192.168.79.102 "/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh"
;;
*)
 echo "Input Args Error..."
;;
esac
```

🎯先加上权限`chmod 777 myhadoop.sh`，再分发脚本`xsync myhadoop.sh`
🎯`myhadoop.sh start`一键启动、`myhadoop.sh stop`一键关停

> 关闭出现以下错误

```

[root@hadoop102 bin]# myhadoop.sh stop
 =================== 关闭 hadoop 集群 ===================
 --------------- 关闭 historyserver ---------------
 --------------- 关闭 yarn ---------------
Stopping nodemanagers
ERROR: Attempting to operate on yarn nodemanager as root
ERROR: but there is no YARN_NODEMANAGER_USER defined. Aborting operation.
Stopping resourcemanager
ERROR: Attempting to operate on yarn resourcemanager as root
ERROR: but there is no YARN_RESOURCEMANAGER_USER defined. Aborting operation.
 --------------- 关闭 hdfs ---------------
Stopping namenodes on [hadoop102]
Stopping datanodes
ERROR: Attempting to operate on hdfs namenode as root
ERROR: but there is no HDFS_NAMENODE_USER defined. Aborting operation.
ERROR: Attempting to operate on hdfs datanode as root
ERROR: but there is no HDFS_DATANODE_USER defined. Aborting operation.
Stopping secondary namenodes [hadoop104]
ERROR: Attempting to operate on hdfs secondarynamenode as root
ERROR: but there is no HDFS_SECONDARYNAMENODE_USER defined. Aborting operation.
```

🎯解决方法：`vim /opt/module/hadoop-3.1.3/etc/hadoop/hadoop-env.sh`

```sh
export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root
```

![image-20230821162233396](img.assets\image-20230821162233396.png)



#### 4.2.6.8 查询所有jps

🎯一键查看所有服务器java进程的脚本，拒绝一台一台jps，`vim /root/bin/jpsall`：

```sh
#!/bin/bash
for host in 192.168.79.102 192.168.79.103 192.168.79.104
do
        echo =============== $host ===============
        ssh $host $JAVA_HOME/bin/jps
done
```

🎯先加上权限`chmod 777 myhadoop.sh`，再分发脚本`xsync jpsall`，最后`jpsall` 即可查看



### 4.2.7 Hadoop常用端口

| 端口名称                   | Hadoop2.x   | Hadoop3.x          |
| -------------------------- | ----------- | ------------------ |
| NameNode 内部通信端口      | 8020 / 9000 | `8020 `/ 9000/9820 |
| NameNode HTTP UI           | `50070`     | `9870`             |
| MapReduce 查看执行任务端口 | 8088        | 8088               |
| 历史服务器通信端口         | 19888       | 19888              |



### 4.2.8 集群时间同步

（**了解即可，影响性能，无需操作**）

如果服务器在公网环境（能连接外网），可以不采用集群时间同步，因为服务器会定期 和公网时间进行校准；

如果服务器在内网环境，必须要配置集群时间同步，否则时间久了，会产生时间偏差， 导致集群执行任务时间不同步



![image-20230821163829434](img.assets\image-20230821163829434.png)

NTPD（Network Time Protocol daemon）是Linux操作系统的一个守护进程，其完整的实现了 NTP 协议，用于校正本地系统与时钟源服务器之前的时间

🎯查看所有节点 ntpd 服务状态和开机自启状态(centos8为chronyd)

```sh
systemctl status ntpd
# 启动 ntpd
systemctl start ntpd
# 查看开机是否自启
systemctl is-enabled ntpd
# 开机自启
systemctl enable ntpd
```

🎯修改 `ntp.conf` 文件，`vim /etc/ntp.conf`

```sh
# 将这一行去掉注释，并修改为自己的网段
restrict 192.168.79.0 mask 255.255.255.0 nomodify notrap

# 集群在局域网中，不使用其他互联网上的时间，注释以下4行
#server 0.centos.pool.ntp.org iburst
#server 1.centos.pool.ntp.org iburst
#server 2.centos.pool.ntp.org iburst
#server 3.centos.pool.ntp.org iburst

# 并在结尾加上（当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步）
server 127.127.1.0
fudge 127.127.1.0 stratum 10
```

🎯修改 192.168.79.102 hadoop102的`/etc/sysconfig/ntpd`，`vim /etc/sysconfig/ntpd`:

```
# 增加(让硬件时间和系统时间一起同步，时间更精准)
SYNC_HWCLOCK=yes
```

🎯重启ntdp服务

```
systemctl start ntdp
```

🎯其他机器配置

```sh
# 关闭所有节点上ntdp服务和自启动
systemctl stop ntdp
systemctl disable ntdp

# 在其他机器上配置每1分钟与时间服务器同步一次
crontab -e
*/1 * * * * /usr/sbin/ntpdate 192.168.79.102
```



### 4.2.9 常见错误

🎯DataNode和NameNode进程同时只能工作一个

![image-20230821164839070](img.assets\image-20230821164839070.png)



🎯8088 端口连接不上

> cat /etc/hosts

注释掉如下代码

```
#127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4
#::1 hadoop102
```

