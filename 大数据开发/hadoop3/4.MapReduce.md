# MapReduce

# 一.MapReduce概述

MapReduce是hadoop的核心组件之一，hadoop要分布式包括两部分，一是分布式文件系统hdfs，一是`分布式计算框架`，就是`mapReduce`，二者缺一不可，也就是说，可以通过mapreduce很容易在hadoop平台上进行分布式的计算编程



## 1.1 定义

MapReduce 是一个分布式运算程序的编程框架，是用户开发“基于 Hadoop 的数据分析应用”的核心框架。

MapReduce 核心功能是将`用户编写的业务逻辑代码`和`自带默认组件`整合成一个完整的`分布式运算程序`，并发运行在一个 Hadoop 集群上。



## 1.2 优缺点

> 优点

1. 易于编程。用户只关心业务逻辑。实现框架的接口
2. 良好扩展性：可以动态增加服务器，解决计算资源不够问题
3. 高容错性。任何一台机器挂掉，可以将任务转移到其他节点
4. 适合海量数据计算（TB/PB）几千台服务器共同计算



> 缺点

1. 不擅长实时计算。MapReduce 无法像 MySQL 一样，在毫秒或者秒级内返回结果。
2. 不擅长流式计算。流式计算的输入数据是动态的，而 MapReduce 的输入数据集是静态的，不能动态变化。 这是因为 MapReduce 自身的设计特点决定了数据源必须是静态的。  --Sparkstreaming flink
3. 不擅长DAG有向无环图计算。多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下， MapReduce 并不是不能做，而是使用后，每个 `MapReduce 作业的输出结果都会写入到磁盘`， 会造成大量的磁盘 IO，导致性能非常的低下。  -- Spark



# 二.MapReduce核心编程思想

![image-20230927144150374](img.assets\image-20230927144150374.png)

1. 分布式的运算程序往往需要分成至少 2 个阶段。
2. 第一个阶段的 MapTask 并发实例，完全并行运行，互不相干。
3. 第二个阶段的 ReduceTask 并发实例互不相干，但是他们的数据依赖于上一个阶段 的所有 MapTask 并发实例的输出。
4. MapReduce 编程模型只能包含一个 Map 阶段和一个 Reduce 阶段，如果用户的业 务逻辑非常复杂，那就只能多个 MapReduce 程序，串行运行。

>“分而治之”是MapReduce的核心思想，它表示把一个大规模的数据集切分成很多小的单独的数据集，然后放在多个机器上同时处理。MapReduce把整个并行运算过程高度抽象到两个函数上，一个是map另一个是reduce。Map函数就是分而治之中的“分”，reduce函数就是分而治之中的“治”



# 三.MapReduce进程

一个完整的MapReduce程序在分布式运行时有三类实例进程

1. MrAppMaster：负责整个程序的过程调度及状态协调（负责任务管理）
2. MapTask：负责Map阶段的整个数据处理流程
3. ReduceTask：负责Reduce阶段的整个数据处理流程



# 四.wordCount源码

>Map类、Reduce类和驱动类。且数据的类型是 Hadoop自身封装的序列化类型

```java
public class WordCount {

  public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable>{
    
    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();
      
    public void map(Object key, Text value, Context context ) throws IOException, InterruptedException {
      StringTokenizer itr = new StringTokenizer(value.toString());
      while (itr.hasMoreTokens()) {
        word.set(itr.nextToken());
        context.write(word, one);
      }
    }
  }
  
  public static class IntSumReducer extends Reducer<Text,IntWritable,Text,IntWritable> {
    private IntWritable result = new IntWritable();

    public void reduce(Text key, Iterable<IntWritable> values, Context context
                       ) throws IOException, InterruptedException {
      int sum = 0;
      for (IntWritable val : values) {
        sum += val.get();
      }
      result.set(sum);
      context.write(key, result);
    }
  }

  public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();
    String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
    if (otherArgs.length < 2) {
      System.err.println("Usage: wordcount <in> [<in>...] <out>");
      System.exit(2);
    }
    Job job = Job.getInstance(conf, "word count");
    job.setJarByClass(WordCount.class);
    job.setMapperClass(TokenizerMapper.class);
    job.setCombinerClass(IntSumReducer.class);
    job.setReducerClass(IntSumReducer.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    for (int i = 0; i < otherArgs.length - 1; ++i) {
      FileInputFormat.addInputPath(job, new Path(otherArgs[i]));
    }
    FileOutputFormat.setOutputPath(job,new Path(otherArgs[otherArgs.length - 1]));
    System.exit(job.waitForCompletion(true) ? 0 : 1);
  }
}
```



# 五.常用数据序列化类型

|  java 类型   | Hadoop Writable 类型 |
| :----------: | :------------------: |
|   Boolean    |   BooleanWritable    |
|     Byte     |     ByteWritable     |
|     Int      |     IntWritable      |
|    Float     |    FloatWritable     |
|     Long     |     LongWritable     |
|    Double    |    DoubleWritable    |
| **`String`** |      **`Text`**      |
|     Map      |     MapWritable      |
|    Array     |    ArrayWritable     |
|     Null     |     NullWritable     |



# 六.MapReduce编程规范

用户编写的程序分成三个部分: Mapper、Reducer 和 Driver

## 6.1 Mapper阶段

```java
  public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable>{
    
    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();
      
    public void map(Object key, Text value, Context context ) throws IOException, InterruptedException {
      StringTokenizer itr = new StringTokenizer(value.toString());
      while (itr.hasMoreTokens()) {
        word.set(itr.nextToken());
        context.write(word, one);
      }
    }
  }
```

- 用户自定义的Mapper要继承自己的父类
- Mapper的输入数据是K-V对的形式(K-V的类型可自定义) --> key是偏移量
- Mapper中的业务逻辑写在map()方法中
- Mapper的输出数据是K-V对的形式(K-V的类型可自定义)
- **map()方法(MapTask进程)对每一个<K,V>调用一次**



## 6.2 Reducer阶段

```java
  public static class IntSumReducer extends Reducer<Text,IntWritable,Text,IntWritable> {
    private IntWritable result = new IntWritable();

    public void reduce(Text key, Iterable<IntWritable> values, Context context
                       ) throws IOException, InterruptedException {
      int sum = 0;
      for (IntWritable val : values) {
        sum += val.get();
      }
      result.set(sum);
      context.write(key, result);
    }
  }
```

- 用户自定义的Reducer要继承自己的父类
- Reducer的输入数据类型对应Mapper的输出数据类型，也是KV
- Reducer的业务逻辑写在reduce()方法中
- **ReduceTask进程对每一组相同k的<k,v>组调用一次reduce()方法**



## 6.3 Driver阶段

相当于YARN集群的客户端，用于提交我们整个程序到YARN集群，提交的是封装了MapReduce程序相关运行参数的job对象

```java
public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();
    String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
    if (otherArgs.length < 2) {
      System.err.println("Usage: wordcount <in> [<in>...] <out>");
      System.exit(2);
    }
    Job job = Job.getInstance(conf, "word count");
    job.setJarByClass(WordCount.class);
    job.setMapperClass(TokenizerMapper.class);
    job.setCombinerClass(IntSumReducer.class);
    job.setReducerClass(IntSumReducer.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    for (int i = 0; i < otherArgs.length - 1; ++i) {
      FileInputFormat.addInputPath(job, new Path(otherArgs[i]));
    }
    FileOutputFormat.setOutputPath(job,new Path(otherArgs[otherArgs.length - 1]));
    System.exit(job.waitForCompletion(true) ? 0 : 1);
  }
```



# 七.自定义编写MapReduce

## 7.1 编写流程

![image-20230927163836030](img.assets\image-20230927163836030.png)

## 7.2 wordCountMapper

```java
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

/**
 * public class Mapper<KEYIN,VALUEIN，KEYOUT,VALUEOUT>
 * KEYIN,map阶段输入key的偏移量：LongWritable
 * VAULEIN,map阶段输入value类型：Text
 * KEYOUT,map阶段输出的key类型：Text
 * VALUEOUT,map阶段输出的value类型：IntWritable
 */
public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {

    private Text text = new Text();

    /**
     * 计数,输出的类型
     */
    private final static IntWritable intWritable = new IntWritable(1);

    @Override
    protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, IntWritable>.Context context) throws IOException, InterruptedException {
        //一行
        String line = value.toString();

        String[] words = line.split(" ");
        for (String word : words) {
            text.set(word);
            context.write(text, intWritable);
        }
    }
}
```

>`context.write()`方法是 Apache Hadoop 中的一个重要方法，主要用于输出结果。它是 org.apache.hadoop.mapreduce.Reducer 类的一个成员方法，通常在 Reducer 的 reduce() 方法中调用。它的作用是`将计算得到的键/值对写入到输出上下文中`，在使用 context.write() 方法之前将键和值序列化为字节数组，因为 Hadoop 中的 MapReduce 框架会将这些字节数组传输到输出端



## 7.3 wordCountReducer

```java
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;

/**
 * public class Reducer<KEYIN,VALUEIN，KEYOUT,VALUEOUT>
 * KEYIN,reduce阶段输入key的类型：Text
 * VAULEIN,reduce阶段输入value类型：LongWritable
 * KEYOUT,reduce阶段输出的key类型：Text
 * VALUEOUT,reduce阶段输出的value类型：IntWritable
 */
public class WordCountReduce extends Reducer<Text, IntWritable, Text, IntWritable> {

    private IntWritable result = new IntWritable();

    @Override
    protected void reduce(Text key, Iterable<IntWritable> values, Reducer<Text, IntWritable, Text, IntWritable>.Context context) throws IOException, InterruptedException {
        int sum = 0;
        for (IntWritable value : values) {
            sum += value.get();
        }
        result.set(sum);
        context.write(key, result);
    }
}
```



## 7.4 wordCountDriver

```java
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

public class WordCountDriver {
    public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {
        //1.创建job对象
        Configuration configuration = new Configuration();
        Job job = Job.getInstance(configuration);

        //2.设置jar包的位置
        job.setJarByClass(WordCountDriver.class);

        //3.关联Map和Reduce
        job.setMapperClass(WordCountMapper.class);
        job.setReducerClass(WordCountReduce.class);

        //4.设置Map的输出类型
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(IntWritable.class);

        //5.设置最后的输出类型
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(Text.class);

        //6.设置输入地址和输出地址
        FileInputFormat.setInputPaths(job, new Path("D:\\1\\12.txt"));
        FileOutputFormat.setOutputPath(job, new Path("D:\\output"));

        //true 记录,方便调试
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
```

>在Hadoop中，一个Job代表一个计算任务，它是Hadoop分布式计算框架中的基本单位。一个Job可以包含多个MapReduce任务，MapReduce是一种用于大规模数据处理的编程模型。当一个Job被提交给Hadoop集群时，Hadoop会将其分解成多个任务（Tasks）并在集群中的各个节点上执行。这些任务包括Map任务和Reduce任务，它们分别执行Map函数和Reduce函数，并通过网络进行数据交换和传输



## 7.5 提交到集群中进行测试

>上面通过Window依赖在Windows环境中运行，但是一般是在Linux环境下运行，故我们需要用maven打包成jar包

> 修改输入路径和输出路径，使其接受传参

```java
// 6.设置输入路径和输出路径
FileInputFormat.setInputPaths(job, new Path(args[0]));
FileOutputFormat.setOutputPath(job, new Path(args[1]));
```

